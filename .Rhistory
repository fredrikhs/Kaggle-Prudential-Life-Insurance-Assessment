install.packages("RWeka")
install.packages("RWekajars")
install.packages("rJava")
install.packages("RWeka")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("RWekajars")
install.packages("RWeka")
require(RWeka)
require(rJava)
require(rJava)
install.packages("rJava")
require(rJava)
require(rJava)
require(rJava)
require(rJava)
x <- seq(1,10)
x
y <- seq(1,10)
y <- c(-890,-1441,-1560,-2220,-2091,-2878,-3268,-3920,-4163,-5471,-5157)
lm(y~x)
length(y)
y <- c(-890,-1441,-1560,-2220,-2091,-2878,-3537,-3268,-3920,-4163,-5471,-5157)
lm(y~x)
lenth(x)
length(x)
length(y
length(y()
length(x)
length(y)
cbind(x,y)
x <- c(1,2,2,3,3,4,5,6,6,6,8,10)
lm(y~x)
require(jsonlite)
data <- fromJSON("R/kaggle/Whats cookin/data/train.json")
require(tm)
corp <- Corpus(data$ingredients)
corp <- Corpus(VectorSource(data$ingredients))
?Corpus
sparse <- TermDocumentMatrix(corp)
head(sparse)
dim(sparse)
sparse <- DocumentTermMatrix(corp)
sparse[1:10,1:10]
inspect(sparse[1:10,1:10])
?DocumentTermMatrix
corp_dtm <- DocumentTermMatrix(corp)
sparse99 <- removeSparseTerms(corp_dtm, .99)
sparse <- removeSparseTerms(corp_dtm, .99)
head(sparse)
dim(sparse)
sparse[1:10,1:10]
inspect(sparse[1:10,1:10])
df <- as.data.frame(as.matrix(sparse))
df$cuisine <- as.factor(data$cuisine)
head(df[1:10,1:10])
str(df)
require(e1071)
?svm
model <- svm(cuisine ~ ., data=df)
summary(model)
dim(test)
dim(df)
test <- df[1:nrow(df)*(3/4),]
train <- df[1:nrow(df)*(3/4),]
train <- df[1:30000,]
test <- df[30001:nrow(df),]
model <- svm(cuisine ~ ., data=df)
require(caret)
install.packages("caret")
require(caret)
model_pred <- predict(model, test)
confusionMatrix(table(model_pred, test$cuisine))
Prudential Life Insurance Assessment
# XGBoost learning
require(xgboost)
require(data.table)
require(Metrics)
require(stringr)
require(Matrix)
require(SparseM)
require(DiagrammeR)
require(Ckmeans.1d.dp)
require(kernlab)
require(randomForest)
require(caret)
setwd("./R/kaggle/Prudential Life Insurance Assessment/")
train <- read.csv("data/train.csv")
test <- read.csv("data/test.csv")
sampleSubmission <- read.csv("data/sample_submission.csv")
str(train)
str(test)
# All features shared, making feature transformations simultaneously.
response <- train$Response
train$training <- 1
test$training  <- 0
data <- rbind(train[-c(1,128)], test[-1])
colnames(data)
prop.table(table(response))
plot(prop.table(table(response)))
feature.names <- names(data[-127])
for( f in feature.names ){
if(class(data[[f]]) == "factor"){
levels <- unique(c(train[[f]],test[[f]]))
train[[f]] <- as.integer(factor(train[[f]]), levels = levels)
test[[f]] <- as.integer(factor(test[[f]]), levels = levels)
data[[f]] <- as.integer(factor(data[[f]]), levels = levels)
}
}
data.roughfix <- na.roughfix(data)
# Using training data to identify most important features with xgboost.
model_xgboost <- xgboost(data = data.matrix(data.roughfix[data.roughfix$training==1,]),
label  = response,
nround  = 55,
objective = "reg:linear",
eval_metric = "rmse",
missing = NaN)
test_xgb_cv <- xgb.cv(params = list("objective" = "reg:linear", "eval_metric" = "rmse"),
data = data.matrix(data.roughfix[data.roughfix$training==1,]),
missing = NaN, nfold=10, label = response,  nrounds = 50)
plot(test_xgb_cv$test.rmse.mean)
plot(test_xgb_cv$test.rmse.mean) # 55 looks like minimum. 1.852797
# [99]	train-rmse:1.587900
model_dump <- xgb.dump(model_xgboost, with.stats = T)
importance.matrix <- xgb.importance(names(data.roughfix), model_xgboost, filename_dump = NULL)
xgb.plot.importance(importance.matrix[1:30])
# Creating a feature counting the medical keywords for each instance (medical keywords is column 80:127)
medkeywords <- apply(data.roughfix[,79:126], 1, sum)
data.roughfix$medkeywords <- as.integer(medkeywords)
partition <- createDataPartition(response, times = 1, p = 0.75)
training <- data.roughfix[data.roughfix$training==1,]
training_train <- training[partition$Resample1,-127]
training_test <- training[-partition$Resample1,-127]
model_xgboost <- xgboost(data = data.matrix(training_train),
label  = response[partition$Resample1],
nround  = 55,
objective = "reg:linear",
eval_metric = "rmse",
missing = NaN)
pred <- predict(model_xgboost, data.matrix(training_test), missing=NaN)
actual <- response[-partition$Resample1]
offset <- lm(actual~pred-1)$coef
data <- data.frame(pred)
data$actual <- actual
head(data)
?optim
data[1:10]
data[1:10,]
data$diff <- data$actual-data$pred
plot(data$diff)
data$diff[1:20]
head(data)
head(data$pred)
summary(dat)
summary(data)
mean(data$diff)
seq(1.5, 7.5, by = 1)
ScoreQuadraticWeightedKappa(pred, actual)
c(min(data$pred), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(data$pred))
SQWKfun = function(x = seq(1.5, 7.5, by = 1), pred) {
preds = data$pred
true = data$actual
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(preds, true, 1, 8)
return(-err)
}
optCuts = optim(seq(1.5, 7.5, by = 1), SQWKfun, pred = data)
require(Hmisc)
install.packages("Hmisc")
install.packages("Hmisc")
install.packages("Hmisc")
require(Hmisc)
optCuts = optim(seq(1.5, 7.5, by = 1), SQWKfun, pred = data)
optCuts
?cut2
c(-Inf, optCuts$par, Inf)
preds = as.numeric(Hmisc::cut2(data$pred, c(-Inf, optCuts$par, Inf)))
preds
data$preds <- preds
head(data)
data$diff2 <- data$actual - data$preds
data[1:20]
data[1:20,]
data$diff <- data$actual-round(data$pred)
data[1:20,]
sum(data$diff)
sum(data$diff2)
dim(data)
preds = as.numeric(Hmisc::cut2(model_pred, c(-Inf, optCuts$par, Inf)))
model <- xgboost(data = data.matrix(data.roughfix[data.roughfix$training == 1,]),
label = response,
nrounds = 55,
objective = "reg:linear",
eval_metric = "rmse",
missing = NaN)
model_pred <- predict(model_xgboost, data.matrix(data.roughfix[data.roughfix$training == 0,]), missing=NaN)
require(xgboost)
require(data.table)
require(Metrics)
require(stringr)
require(Matrix)
require(SparseM)
require(DiagrammeR)
require(Ckmeans.1d.dp)
require(kernlab)
require(randomForest)
require(caret)
model <- xgboost(data = data.matrix(data.roughfix[data.roughfix$training == 1,]),
label = response,
nrounds = 55,
objective = "reg:linear",
eval_metric = "rmse",
missing = NaN)
model_pred <- predict(model_xgboost, data.matrix(data.roughfix[data.roughfix$training == 0,]), missing=NaN)
preds = as.numeric(Hmisc::cut2(model_pred, c(-Inf, optCuts$par, Inf)))
results2 <- cbind(test$Id, preds)
head(sampleSubmission)
colnames(results2) <- c("Id","Response")
head(results2)
write.table(results2, "results2.csv", row.names=F, sep=",")
